{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Title  \\\n",
      "0       Oscar et la dame rose (2009)    \n",
      "1                       Cupid (1997)    \n",
      "2   Young, Wild and Wonderful (1980)    \n",
      "3              The Secret Sin (1915)    \n",
      "4             The Unrecovered (2007)    \n",
      "\n",
      "                                                Plot       Genre  \n",
      "0   Listening in to a conversation between his do...      drama   \n",
      "1   A brother and sister with a past incestuous r...   thriller   \n",
      "2   As the bus empties the students for their fie...      adult   \n",
      "3   To help their unemployed father make ends mee...      drama   \n",
      "4   The film's title refers not only to the un-re...      drama   \n",
      "                           Title  \\\n",
      "0          Edgar's Lunch (1998)    \n",
      "1      La guerra de papá (1977)    \n",
      "2   Off the Beaten Track (2010)    \n",
      "3        Meu Amigo Hindu (2015)    \n",
      "4             Er nu zhai (1955)    \n",
      "\n",
      "                                                Plot          Genre  \n",
      "0   L.R. Brane loves his life - his car, his apar...      thriller   \n",
      "1   Spain, March 1964: Quico is a very naughty ch...        comedy   \n",
      "2   One year in the life of Albin and his family ...   documentary   \n",
      "3   His father has died, he hasn't spoken with hi...         drama   \n",
      "4   Before he was known internationally as a mart...         drama   \n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/Users/parthsdambhare/Documents/Enc/Movie classifier/Genre Classification Dataset/train_data.txt'\n",
    "test_data_path = '/Users/parthsdambhare/Documents/Enc/Movie classifier/Genre Classification Dataset/test_data_solution.txt'\n",
    "\n",
    "# Load train and test data\n",
    "movies_train = []\n",
    "with open('/Users/parthsdambhare/Documents/Enc/Movie classifier/Genre Classification Dataset/train_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(':::') \n",
    "        title = parts[1]\n",
    "        plot = parts[3]\n",
    "        genre = parts[2] \n",
    "        movies_train.append([title , plot, genre])\n",
    "        \n",
    "movies_test = []\n",
    "with open('/Users/parthsdambhare/Documents/Enc/Movie classifier/Genre Classification Dataset/test_data_solution.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(':::') \n",
    "        title = parts[1]\n",
    "        plot = parts[3]\n",
    "        genre = parts[2] \n",
    "        movies_test.append([title, plot, genre])        \n",
    "\n",
    "train_df = pd.DataFrame(movies_train, columns=['Title', 'Plot', 'Genre'])\n",
    "print(train_df.head())\n",
    "\n",
    "test_df = pd.DataFrame(movies_test, columns=['Title', 'Plot', 'Genre'])\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Encode genres into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train['Genre'] = label_encoder.fit_transform(df_train['Genre'])\n",
    "df_test['Genre'] = label_encoder.transform(df_test['Genre'])\n",
    "\n",
    "# Apply TF-IDF Vectorizer to the plot summaries\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "X_train = tfidf.fit_transform(df_train['Plot']).toarray()\n",
    "X_test = tfidf.transform(df_test['Plot']).toarray()\n",
    "\n",
    "y_train = df_train['Genre'].values\n",
    "y_test = df_test['Genre'].values'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the genres into numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Genre'] = label_encoder.fit_transform(train_df['Genre'])\n",
    "test_df['Genre'] = label_encoder.transform(test_df['Genre'])\n",
    "\n",
    "# Train-validation split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['Plot'], train_df['Genre'], test_size=0.2, random_state=42\n",
    ")\n",
    "test_texts = test_df['Plot']\n",
    "test_labels = test_df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_data(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(), \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "# Tokenize train, validation, and test data\n",
    "train_encodings = tokenize_data(train_texts)\n",
    "val_encodings = tokenize_data(val_texts)\n",
    "test_encodings = tokenize_data(test_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieGenreDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = MovieGenreDataset(train_encodings, train_labels.values)\n",
    "val_dataset = MovieGenreDataset(val_encodings, val_labels.values)\n",
    "test_dataset = MovieGenreDataset(test_encodings, test_labels.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=27, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "import torch\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "# Move the model to MPS (Apple Silicon)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "#optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2711 [00:00<?, ?it/s]/var/folders/mc/gvm9y0s93y50g3gglym7mpw40000gn/T/ipykernel_5160/360674222.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|██████████| 2711/2711 [40:34<00:00,  1.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Training loss: 1.3705\n",
      "Validation loss: 1.2300, Validation accuracy: 0.6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2711/2711 [2:13:46<00:00,  2.96s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training loss: 0.9341\n",
      "Validation loss: 1.1964, Validation accuracy: 0.6427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2711/2711 [3:30:36<00:00,  4.66s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Training loss: 0.5860\n",
      "Validation loss: 1.4111, Validation accuracy: 0.6235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Move batch to the device\n",
    "        batch = {key: val.to(device) for key, val in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "            total += len(batch['labels'])\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {ac curacy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
